{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\gh\\anaconda3\\envs\\ai_endpoint\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import dashscope\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "from dashscope.audio.asr import Recognition\n",
    "from dashscope.audio.tts_v2 import *\n",
    "\n",
    "# API-Key\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "os.environ[\"ALIYUN_API_KEY\"] = \"sk-4d9f8ebafb104f5dadbafa4eeca93e5b\"\n",
    "dashscope.api_key = \"sk-4d9f8ebafb104f5dadbafa4eeca93e5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gh\\anaconda3\\envs\\ai_endpoint\\lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:197: UserWarning: Found nvidia/llama-3.1-nemotron-70b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 初始化大模型\n",
    "instruct_chat = ChatNVIDIA(model=\"nvidia/llama-3.1-nemotron-70b-instruct\")\n",
    "instruct_llm = instruct_chat | StrOutputParser()\n",
    "\n",
    "# 定义提示模板\n",
    "prompt_template = ChatPromptTemplate.from_template(\"以下是与问题相关的信息：{context}\\n用户需求：{input}\\n请给出对应的无人机的回答\")\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text_content = soup.get_text()\n",
    "        text_content = ' '.join(text_content.split())\n",
    "        return text_content\n",
    "    else:\n",
    "        raise Exception(f\"请求失败，状态码: {response.status_code}\")\n",
    "\n",
    "def load_local_knowledge(base_path):\n",
    "    loader = TextLoader(base_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    embeddings = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def load_web_knowledge(url):\n",
    "    text_content = fetch_text_from_url(url)\n",
    "    with open('./txt/webpage_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(text_content)\n",
    "    return load_local_knowledge('./txt/webpage_content.txt')\n",
    "\n",
    "def retrieve_from_knowledgebase(vectorstore, query, k=3):\n",
    "    relevant_docs = vectorstore.similarity_search(query, k=k)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    return context\n",
    "\n",
    "def process_user_input(chat_history, source_type, source_path_or_url, input_text):\n",
    "    try:\n",
    "        if source_type not in [\"URL\", \"文件路径\"]:\n",
    "            raise ValueError(\"无效的知识来源类型，请选择 'URL' 或 '文件路径'。\")\n",
    "        if source_type == \"URL\":\n",
    "            vectorstore = load_web_knowledge(source_path_or_url)\n",
    "        elif source_type == \"文件路径\":\n",
    "            vectorstore = load_local_knowledge(source_path_or_url)\n",
    "        context = retrieve_from_knowledgebase(vectorstore, input_text)\n",
    "        prompt = prompt_template.format(context=context, input=input_text)\n",
    "        answer = instruct_llm.invoke(prompt)\n",
    "        chat_history = chat_history + [(input_text, answer)]\n",
    "        return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        chat_history = chat_history + [(None, f\"发生错误: {str(e)}\")]\n",
    "        return \"\", chat_history\n",
    "    \n",
    "def transcribe_audio(audio_path):\n",
    "    _, sample_rate = sf.read(audio_path)\n",
    "    # 假设Recognition类已定义并可以调用\n",
    "    recognition = Recognition(\n",
    "        model='paraformer-realtime-v2',\n",
    "        format='wav',\n",
    "        sample_rate=sample_rate,\n",
    "        language_hints=['zh', 'en'],\n",
    "        callback=None\n",
    "    )\n",
    "    result = recognition.call(audio_path)\n",
    "    sentences = result.output['sentence']\n",
    "    original_text = [sentence['text'] for sentence in sentences][0]\n",
    "    return original_text\n",
    "\n",
    "# 处理音频输入\n",
    "def process_audio_input(chat_history, source_type, source_path_or_url, audio_file):\n",
    "    try:\n",
    "        # 验证 source_type 是否有效\n",
    "        if source_type not in [\"URL\", \"文件路径\"]:\n",
    "            raise ValueError(\"无效的知识来源类型，请选择 'URL' 或 '文件路径'。\")\n",
    "\n",
    "        if audio_file:\n",
    "            input_text = transcribe_audio(audio_file)\n",
    "            return process_user_input(chat_history, source_type, source_path_or_url, input_text)\n",
    "        else:\n",
    "            return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        # 捕获异常并在聊天历史中添加错误信息\n",
    "        chat_history.append((None, f\"发生错误: {str(e)}\"))\n",
    "        return \"\", chat_history\n",
    "\n",
    "def analyze_image(image_path):\n",
    "    # 图像分析逻辑...\n",
    "    invoke_url = \"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\"\n",
    "    stream = False\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "        \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": 'meta/llama-3.2-11b-vision-instruct',\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'''Here is an image related to crop growth. Please examine the plants and their growing environment in this picture. If there are any issues, please briefly describe the problem and suggest a quick solution. If there are no problems, please state that the plants are growing well. <img src=\"data:image/png;base64,{image_b64}\" />'''\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 1.00,\n",
    "        \"top_p\": 1.00,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "        result = response.json()['choices'][0]['message']['content']\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_chinese(text):\n",
    "    \"\"\"\n",
    "    将文本翻译成中文。\n",
    "    \n",
    "    :param text: 需要翻译的英文文本\n",
    "    :return: 翻译后的中文文本\n",
    "    \"\"\"\n",
    "    # pic_read = ChatNVIDIA(model=\"thudm/chatglm3-6b\")\n",
    "    pic_read = ChatNVIDIA(model=\"baichuan-inc/baichuan2-13b-chat\")\n",
    "    pic_prompt_template = ChatPromptTemplate.from_template(\"请把 {input} 翻译成中文，简单总结并优化格式。\")\n",
    "    pic_chain = pic_prompt_template | pic_read | StrOutputParser()\n",
    "    translated_text = pic_chain.invoke(text)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "def analyze_and_translate(image_path):\n",
    "    \"\"\"\n",
    "    分析图片并将其结果翻译成中文。\n",
    "    \n",
    "    :param image_path: 图片文件路径\n",
    "    :return: 翻译后的中文结果\n",
    "    \"\"\"\n",
    "    analysis_result = analyze_image(image_path)\n",
    "    translated_result = translate_to_chinese(analysis_result)\n",
    "    return translated_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gh\\anaconda3\\envs\\ai_endpoint\\lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "welcome_message = \"您好！欢迎使用无人机综合智能检索系统。请告诉我您的问题、上传音频文件或图片以开始。\"\n",
    "\n",
    "with gr.Blocks(css=\"\"\"\n",
    "    .footer {text-align: center;}\n",
    "    .submit-btn {margin-left: 10px; padding: 5px 10px; font-size: 0.8em; height: 40px; width: 60px;}\n",
    "    .clear-btn {margin-left: 10px; padding: 5px 10px; font-size: 0.8em; height: 40px; width: 120px;}\n",
    "    .input-container {display: flex; align-items: flex-end;} /* 使输入框和按钮在同一行并且对齐 */\n",
    "    .input-box {flex-grow: 1;} /* 使输入框占据更多的空间 */\n",
    "\"\"\") as demo:\n",
    "    gr.Markdown(\"## 🚀 无人机综合智能检索系统\")\n",
    "    gr.Markdown(\"输入URL或文件路径以加载知识并开始聊天。\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        source_type = gr.Radio(choices=[\"URL\", \"文件路径\"], label=\"知识来源类型\", value=\"文件路径\")\n",
    "        source_path_or_url = gr.Textbox(label=\"知识来源URL或文件路径\", value=\"./txt/dji_webpage_content.txt\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(value=[(None, welcome_message)], elem_classes=\"chatbox-container\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(elem_classes=\"input-container\"):\n",
    "            submit_btn = gr.Button(\"提交\", variant=\"primary\", size=\"sm\", elem_classes=\"submit-btn\")\n",
    "            msg = gr.Textbox(label=\"用户查询\", placeholder=\"在这里输入您的问题... 📝\", elem_classes=\"input-box\")\n",
    "                \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"上传音频\", type=\"filepath\", elem_classes=\"audio-uploader\")\n",
    "        image_input = gr.Image(type=\"filepath\", label=\"上传图片\", elem_classes=\"image-uploader\")\n",
    "    \n",
    "    def process_image_input(chat_history, image_path):\n",
    "        analysis_result = analyze_and_translate(image_path)\n",
    "        chat_history = chat_history + [(\"分析结果:\", analysis_result)]\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    def clear_chat_history():\n",
    "        # 确保返回的数据格式是正确的\n",
    "        return [], [(None, welcome_message)]\n",
    "\n",
    "    submit_btn.click(fn=lambda x, y, z, w: process_user_input(x, y, z, w), inputs=[chatbot, source_type, source_path_or_url, msg], outputs=[msg, chatbot])\n",
    "    audio_input.change(fn=lambda x: process_audio_input(chatbot.value, source_type.value, source_path_or_url.value, x), inputs=[audio_input], outputs=[msg, chatbot])\n",
    "    image_input.upload(fn=lambda x: process_image_input(chatbot.value, x), inputs=[image_input], outputs=[msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_endpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
