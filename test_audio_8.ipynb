{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\gh\\anaconda3\\envs\\ai_endpoint\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader, WebBaseLoader\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import dashscope\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "from dashscope.audio.asr import Recognition\n",
    "from dashscope.audio.tts_v2 import *\n",
    "\n",
    "# API-Key\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"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\"\n",
    "os.environ[\"ALIYUN_API_KEY\"] = \"sk-4d9f8ebafb104f5dadbafa4eeca93e5b\"\n",
    "dashscope.api_key = \"sk-4d9f8ebafb104f5dadbafa4eeca93e5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gh\\anaconda3\\envs\\ai_endpoint\\lib\\site-packages\\langchain_nvidia_ai_endpoints\\_common.py:197: UserWarning: Found nvidia/llama-3.1-nemotron-70b-instruct in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–å¤§æ¨¡å‹\n",
    "instruct_chat = ChatNVIDIA(model=\"nvidia/llama-3.1-nemotron-70b-instruct\")\n",
    "instruct_llm = instruct_chat | StrOutputParser()\n",
    "\n",
    "# å®šä¹‰æç¤ºæ¨¡æ¿\n",
    "prompt_template = ChatPromptTemplate.from_template(\"ä»¥ä¸‹æ˜¯ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ï¼š{context}\\nç”¨æˆ·éœ€æ±‚ï¼š{input}\\nè¯·ç»™å‡ºå¯¹åº”çš„æ— äººæœºçš„å›ç­”\")\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text_content = soup.get_text()\n",
    "        text_content = ' '.join(text_content.split())\n",
    "        return text_content\n",
    "    else:\n",
    "        raise Exception(f\"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}\")\n",
    "\n",
    "def load_local_knowledge(base_path):\n",
    "    loader = TextLoader(base_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    embeddings = NVIDIAEmbeddings(model=\"nvidia/llama-3.2-nv-embedqa-1b-v2\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "def load_web_knowledge(url):\n",
    "    text_content = fetch_text_from_url(url)\n",
    "    with open('./txt/webpage_content.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(text_content)\n",
    "    return load_local_knowledge('./txt/webpage_content.txt')\n",
    "\n",
    "def retrieve_from_knowledgebase(vectorstore, query, k=3):\n",
    "    relevant_docs = vectorstore.similarity_search(query, k=k)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    return context\n",
    "\n",
    "def process_user_input(chat_history, source_type, source_path_or_url, input_text):\n",
    "    try:\n",
    "        if source_type not in [\"URL\", \"æ–‡ä»¶è·¯å¾„\"]:\n",
    "            raise ValueError(\"æ— æ•ˆçš„çŸ¥è¯†æ¥æºç±»å‹ï¼Œè¯·é€‰æ‹© 'URL' æˆ– 'æ–‡ä»¶è·¯å¾„'ã€‚\")\n",
    "        if source_type == \"URL\":\n",
    "            vectorstore = load_web_knowledge(source_path_or_url)\n",
    "        elif source_type == \"æ–‡ä»¶è·¯å¾„\":\n",
    "            vectorstore = load_local_knowledge(source_path_or_url)\n",
    "        context = retrieve_from_knowledgebase(vectorstore, input_text)\n",
    "        prompt = prompt_template.format(context=context, input=input_text)\n",
    "        answer = instruct_llm.invoke(prompt)\n",
    "        chat_history = chat_history + [(input_text, answer)]\n",
    "        return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        chat_history = chat_history + [(None, f\"å‘ç”Ÿé”™è¯¯: {str(e)}\")]\n",
    "        return \"\", chat_history\n",
    "    \n",
    "def transcribe_audio(audio_path):\n",
    "    _, sample_rate = sf.read(audio_path)\n",
    "    # å‡è®¾Recognitionç±»å·²å®šä¹‰å¹¶å¯ä»¥è°ƒç”¨\n",
    "    recognition = Recognition(\n",
    "        model='paraformer-realtime-v2',\n",
    "        format='wav',\n",
    "        sample_rate=sample_rate,\n",
    "        language_hints=['zh', 'en'],\n",
    "        callback=None\n",
    "    )\n",
    "    result = recognition.call(audio_path)\n",
    "    sentences = result.output['sentence']\n",
    "    original_text = [sentence['text'] for sentence in sentences][0]\n",
    "    return original_text\n",
    "\n",
    "# å¤„ç†éŸ³é¢‘è¾“å…¥\n",
    "def process_audio_input(chat_history, source_type, source_path_or_url, audio_file):\n",
    "    try:\n",
    "        # éªŒè¯ source_type æ˜¯å¦æœ‰æ•ˆ\n",
    "        if source_type not in [\"URL\", \"æ–‡ä»¶è·¯å¾„\"]:\n",
    "            raise ValueError(\"æ— æ•ˆçš„çŸ¥è¯†æ¥æºç±»å‹ï¼Œè¯·é€‰æ‹© 'URL' æˆ– 'æ–‡ä»¶è·¯å¾„'ã€‚\")\n",
    "\n",
    "        if audio_file:\n",
    "            input_text = transcribe_audio(audio_file)\n",
    "            return process_user_input(chat_history, source_type, source_path_or_url, input_text)\n",
    "        else:\n",
    "            return \"\", chat_history\n",
    "    except Exception as e:\n",
    "        # æ•è·å¼‚å¸¸å¹¶åœ¨èŠå¤©å†å²ä¸­æ·»åŠ é”™è¯¯ä¿¡æ¯\n",
    "        chat_history.append((None, f\"å‘ç”Ÿé”™è¯¯: {str(e)}\"))\n",
    "        return \"\", chat_history\n",
    "\n",
    "def analyze_image(image_path):\n",
    "    # å›¾åƒåˆ†æé€»è¾‘...\n",
    "    invoke_url = \"https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions\"\n",
    "    stream = False\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_b64 = base64.b64encode(f.read()).decode()\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ.get('NVIDIA_API_KEY')}\",\n",
    "        \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": 'meta/llama-3.2-11b-vision-instruct',\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f'''Here is an image related to crop growth. Please examine the plants and their growing environment in this picture. If there are any issues, please briefly describe the problem and suggest a quick solution. If there are no problems, please state that the plants are growing well. <img src=\"data:image/png;base64,{image_b64}\" />'''\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 1.00,\n",
    "        \"top_p\": 1.00,\n",
    "        \"stream\": stream\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "        result = response.json()['choices'][0]['message']['content']\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_chinese(text):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ã€‚\n",
    "    \n",
    "    :param text: éœ€è¦ç¿»è¯‘çš„è‹±æ–‡æ–‡æœ¬\n",
    "    :return: ç¿»è¯‘åçš„ä¸­æ–‡æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    # pic_read = ChatNVIDIA(model=\"thudm/chatglm3-6b\")\n",
    "    pic_read = ChatNVIDIA(model=\"baichuan-inc/baichuan2-13b-chat\")\n",
    "    pic_prompt_template = ChatPromptTemplate.from_template(\"è¯·æŠŠ {input} ç¿»è¯‘æˆä¸­æ–‡ï¼Œç®€å•æ€»ç»“å¹¶ä¼˜åŒ–æ ¼å¼ã€‚\")\n",
    "    pic_chain = pic_prompt_template | pic_read | StrOutputParser()\n",
    "    translated_text = pic_chain.invoke(text)\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "def analyze_and_translate(image_path):\n",
    "    \"\"\"\n",
    "    åˆ†æå›¾ç‰‡å¹¶å°†å…¶ç»“æœç¿»è¯‘æˆä¸­æ–‡ã€‚\n",
    "    \n",
    "    :param image_path: å›¾ç‰‡æ–‡ä»¶è·¯å¾„\n",
    "    :return: ç¿»è¯‘åçš„ä¸­æ–‡ç»“æœ\n",
    "    \"\"\"\n",
    "    analysis_result = analyze_image(image_path)\n",
    "    translated_result = translate_to_chinese(analysis_result)\n",
    "    return translated_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gh\\anaconda3\\envs\\ai_endpoint\\lib\\site-packages\\gradio\\analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.41.0, however version 4.44.1 is available, please upgrade. \n",
      "--------\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "welcome_message = \"æ‚¨å¥½ï¼æ¬¢è¿ä½¿ç”¨æ— äººæœºç»¼åˆæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨çš„é—®é¢˜ã€ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶æˆ–å›¾ç‰‡ä»¥å¼€å§‹ã€‚\"\n",
    "\n",
    "with gr.Blocks(css=\"\"\"\n",
    "    .footer {text-align: center;}\n",
    "    .submit-btn {margin-left: 10px; padding: 5px 10px; font-size: 0.8em; height: 40px; width: 60px;}\n",
    "    .clear-btn {margin-left: 10px; padding: 5px 10px; font-size: 0.8em; height: 40px; width: 120px;}\n",
    "    .input-container {display: flex; align-items: flex-end;} /* ä½¿è¾“å…¥æ¡†å’ŒæŒ‰é’®åœ¨åŒä¸€è¡Œå¹¶ä¸”å¯¹é½ */\n",
    "    .input-box {flex-grow: 1;} /* ä½¿è¾“å…¥æ¡†å æ®æ›´å¤šçš„ç©ºé—´ */\n",
    "\"\"\") as demo:\n",
    "    gr.Markdown(\"## ğŸš€ æ— äººæœºç»¼åˆæ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ\")\n",
    "    gr.Markdown(\"è¾“å…¥URLæˆ–æ–‡ä»¶è·¯å¾„ä»¥åŠ è½½çŸ¥è¯†å¹¶å¼€å§‹èŠå¤©ã€‚\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        source_type = gr.Radio(choices=[\"URL\", \"æ–‡ä»¶è·¯å¾„\"], label=\"çŸ¥è¯†æ¥æºç±»å‹\", value=\"æ–‡ä»¶è·¯å¾„\")\n",
    "        source_path_or_url = gr.Textbox(label=\"çŸ¥è¯†æ¥æºURLæˆ–æ–‡ä»¶è·¯å¾„\", value=\"./txt/dji_webpage_content.txt\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(value=[(None, welcome_message)], elem_classes=\"chatbox-container\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(elem_classes=\"input-container\"):\n",
    "            submit_btn = gr.Button(\"æäº¤\", variant=\"primary\", size=\"sm\", elem_classes=\"submit-btn\")\n",
    "            msg = gr.Textbox(label=\"ç”¨æˆ·æŸ¥è¯¢\", placeholder=\"åœ¨è¿™é‡Œè¾“å…¥æ‚¨çš„é—®é¢˜... ğŸ“\", elem_classes=\"input-box\")\n",
    "                \n",
    "    with gr.Row():\n",
    "        audio_input = gr.Audio(label=\"ä¸Šä¼ éŸ³é¢‘\", type=\"filepath\", elem_classes=\"audio-uploader\")\n",
    "        image_input = gr.Image(type=\"filepath\", label=\"ä¸Šä¼ å›¾ç‰‡\", elem_classes=\"image-uploader\")\n",
    "    \n",
    "    def process_image_input(chat_history, image_path):\n",
    "        analysis_result = analyze_and_translate(image_path)\n",
    "        chat_history = chat_history + [(\"åˆ†æç»“æœ:\", analysis_result)]\n",
    "        return \"\", chat_history\n",
    "    \n",
    "    def clear_chat_history():\n",
    "        # ç¡®ä¿è¿”å›çš„æ•°æ®æ ¼å¼æ˜¯æ­£ç¡®çš„\n",
    "        return [], [(None, welcome_message)]\n",
    "\n",
    "    submit_btn.click(fn=lambda x, y, z, w: process_user_input(x, y, z, w), inputs=[chatbot, source_type, source_path_or_url, msg], outputs=[msg, chatbot])\n",
    "    audio_input.change(fn=lambda x: process_audio_input(chatbot.value, source_type.value, source_path_or_url.value, x), inputs=[audio_input], outputs=[msg, chatbot])\n",
    "    image_input.upload(fn=lambda x: process_image_input(chatbot.value, x), inputs=[image_input], outputs=[msg, chatbot])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_endpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
